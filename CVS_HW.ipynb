{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "CVS_HW_final.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VJdk2bW_oA4Y",
        "colab_type": "text"
      },
      "source": [
        "# CVS HW"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qsUibpK7T4Al",
        "colab_type": "text"
      },
      "source": [
        "Add here the path for the videos, then 'run all'.\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "k4YfOiueT9E0",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "path = '/content/HW/'"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lU9TO8R_QkfY",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!gdown https://drive.google.com/uc?id=1Lnjdb48lx9urYtETpDBZc8wM7iNY-QCd\n",
        "!gdown https://drive.google.com/uc?id=1grVMSVkhGplNKDOE7XOzF-RrUe_EkJRF"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "iOeG_0raOE4F",
        "colab": {}
      },
      "source": [
        "!git clone https://github.com/AlexeyAB/darknet"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lNHATDXMcShX",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "%cd darknet"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "MwDPxds6OJAD",
        "colab": {}
      },
      "source": [
        "# change makefile to have GPU and OPENCV enabled\n",
        "!sed -i 's/OPENCV=0/OPENCV=1/' Makefile\n",
        "!sed -i 's/GPU=0/GPU=1/' Makefile\n",
        "!sed -i 's/CUDNN=0/CUDNN=1/' Makefile\n",
        "!make"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xDW29y_5QNwk",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!cp cfg/yolov3.cfg cfg/yolov3_testing.cfg\n",
        "\n",
        "!sed -i 's/max_batches = 500200/max_batches = 16000/' cfg/yolov3_testing.cfg\n",
        "!sed -i 's/steps=400000,450000/steps=12800,14400/' cfg/yolov3_testing.cfg\n",
        "!sed -i '610 s@classes=80@classes=8@' cfg/yolov3_testing.cfg\n",
        "!sed -i '696 s@classes=80@classes=8@' cfg/yolov3_testing.cfg\n",
        "!sed -i '783 s@classes=80@classes=8@' cfg/yolov3_testing.cfg\n",
        "!sed -i '603 s@filters=255@filters=39@' cfg/yolov3_testing.cfg\n",
        "!sed -i '689 s@filters=255@filters=39@' cfg/yolov3_testing.cfg\n",
        "!sed -i '776 s@filters=255@filters=39@' cfg/yolov3_testing.cfg\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "g15t05oNDOwH",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "%cd .."
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ib41NCrXDJoL",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Homework dataset\n",
        "!wget http://deeplearning.iit.bme.hu/CVS/HW.zip\n",
        "!unzip -qq HW.zip\n",
        "!rm HW.zip"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "S63FCYbYAsVP",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import cv2\n",
        "from torchvision import datasets, transforms, models\n",
        "from PIL import Image\n",
        "from google.colab.patches import cv2_imshow\n",
        "\n",
        "traffic_classes = ['Bump', 'Bumpy road', 'Bus stop', 'Children', 'Crossing (blue)', 'Crossing (red)', 'Cyclists',\n",
        "     'Danger (other)', 'Dangerous left turn', 'Dangerous right turn', 'Give way', 'Go ahead', 'Go ahead or left',\n",
        "     'Go ahead or right', 'Go around either way', 'Go around left', 'Go around right', 'Intersection', 'Limit 100',\n",
        "     'Limit 120', 'Limit 20', 'Limit 30', 'Limit 50', 'Limit 60', 'Limit 70', 'Limit 80', 'Limit 80 over',\n",
        "     'Limit over', 'Main road', 'Main road over', 'Multiple dangerous turns', 'Narrow road (left)',\n",
        "     'Narrow road (right)', 'No entry', 'No entry (both directions)', 'No entry (truck)', 'No stopping', 'No takeover',\n",
        "     'No takeover (truck)', 'No takeover (truck) end', 'No takeover end', 'No waiting', 'One way road',\n",
        "     'Parking', 'Road works', 'Roundabout', 'Slippery road', 'Stop', 'Traffic light', 'Train crossing',\n",
        "     'Train crossing (no barrier)', 'Wild animals', 'X - Priority','X - Turn Left', 'X - Turn Right']\n",
        "\n",
        "# Convolutional module (Conv+ReLU+BatchNorm)\n",
        "class Conv(nn.Module):\n",
        "    \n",
        "    # Constructor gets in and output channels and stride\n",
        "    def __init__(self, in_channels, channels, stride=1):\n",
        "        super(Conv, self).__init__()\n",
        "        \n",
        "        # Create 2D Convolution (3x3)\n",
        "        self.conv = nn.Conv2d(in_channels, channels, kernel_size=3, stride=stride, padding=1, bias=False)\n",
        "        \n",
        "        # Create Batchnorm\n",
        "        self.bn = nn.BatchNorm2d(channels)\n",
        "        \n",
        "    # Overwrite forward\n",
        "    def forward(self,x):\n",
        "        # Call the layers in the proper order\n",
        "        return self.bn(torch.relu(self.conv(x)))\n",
        "\n",
        "class ConvNet(nn.Module):\n",
        "    \n",
        "    # Constructor gets channel number of the image and the first filter\n",
        "    def __init__(self, base_channels=16, in_channels=3, num_classes=55):\n",
        "        super(ConvNet, self).__init__()\n",
        "        \n",
        "        # First two filters\n",
        "        self.c11 = Conv(in_channels, base_channels)\n",
        "        self.c12 = Conv(base_channels, base_channels)\n",
        "        \n",
        "        # Downscale using strided convolution and expand channels\n",
        "        self.d1 = Conv(base_channels, base_channels*2, 2)\n",
        "        \n",
        "        # Repeat this 4 times\n",
        "        self.c21 = Conv(base_channels*2, base_channels*2)\n",
        "        self.c22 = Conv(base_channels*2, base_channels*2)\n",
        "        \n",
        "        self.d2 = Conv(base_channels*2, base_channels*4, 2)\n",
        "        \n",
        "        self.c31 = Conv(base_channels*4, base_channels*4)\n",
        "        self.c32 = Conv(base_channels*4, base_channels*4)\n",
        "        \n",
        "        self.d3 = Conv(base_channels*4, base_channels*8, 2)\n",
        "        \n",
        "        self.c41 = Conv(base_channels*8, base_channels*8)\n",
        "        self.c42 = Conv(base_channels*8, base_channels*8)\n",
        "        \n",
        "        self.d4 = Conv(base_channels*8, base_channels*16, 2)\n",
        "        \n",
        "        self.c51 = Conv(base_channels*16, base_channels*16)\n",
        "        self.c52 = Conv(base_channels*16, base_channels*16)\n",
        "        \n",
        "        # Input image is 32x32 -> after 5 downscaling the activation map is 1x1\n",
        "        self.d5 = Conv(base_channels*16, base_channels*32, 2)\n",
        "        \n",
        "        # Classifier is a normal 1x1 convolution that produces num_classes class scores\n",
        "        # This layer does not have BatchNorm of ReLU\n",
        "        self.classifier = nn.Conv2d(base_channels*32,num_classes,kernel_size=1)\n",
        "        \n",
        "    def forward(self,x):\n",
        "        # Class all the layers\n",
        "        x = self.d1(self.c12(self.c11(x)))\n",
        "        x = self.d2(self.c22(self.c21(x)))\n",
        "        x = self.d3(self.c32(self.c31(x)))\n",
        "        x = self.d4(self.c42(self.c41(x)))\n",
        "        x = self.d5(self.c52(self.c51(x)))\n",
        "        \n",
        "        # Squeeze removes dimensions that have only 1 element\n",
        "        # Output of the conv layer is (batch_size x num_classes x 1 x 1)\n",
        "        # After squeeze is becomes (batch_size x num_classes)\n",
        "        return torch.squeeze(self.classifier(x))\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "model=torch.load('/content/traffic_sign_model_HC_final.pth')\n",
        "model.eval()\n",
        "\n",
        "imsize = 32\n",
        "loader = transforms.Compose([transforms.Scale(imsize), transforms.ToTensor()])\n",
        "                         \n",
        "def predict_traffic(model, x,y,w,h, image, classes):\n",
        "  img = image[y:y+h, x:x+w]\n",
        "  img = cv2.resize(img,(32,32))\n",
        "  imsize = 32\n",
        "  loader = transforms.Compose([transforms.Scale(imsize), transforms.ToTensor()]) \n",
        "\n",
        "  img = Image.fromarray(img)\n",
        "  img = loader(img).float()\n",
        "  img = img.unsqueeze(0) \n",
        "  img = img.cuda()\n",
        "\n",
        "  output = model(img)\n",
        "  output = list(output)\n",
        "  lst = sorted(output)\n",
        "  pred = max(output)\n",
        "  name = traffic_classes[output.index(pred)]\n",
        "  return name"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "n0vSBJVDkix_",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import cv2\n",
        "import numpy as np\n",
        "import glob\n",
        "import random\n",
        "from google.colab.patches import cv2_imshow\n",
        "from scipy.optimize import fsolve\n",
        "import math\n",
        "import os\n",
        "\n",
        "# Load Yolo\n",
        "net = cv2.dnn.readNet(\"/content/yolov3_training_1600.weights\", \"/content/darknet/cfg/yolov3_testing.cfg\")\n",
        "\n",
        "def equations(p):\n",
        "  xp, yp = p\n",
        "  return ((6.0604821777343750e+02)*xp+(3.1442626953125000e+02)*z-u*z,(6.0498577880859375e+02)*yp+(2.4605038452148438e+02)*z-v*z)\n",
        "\n",
        "# Name custom object\n",
        "classes = [\"SUV\",\"plane\",\"traffic sign\",\"angry\",\"evil\",\"happy\",\"sad\",\"truck\"]\n",
        "\n",
        "classNames = ['traffic sign', 'vehicle', 'cactus']\n",
        "subclassNames = [\n",
        "    ['Bump', 'Bumpy road', 'Bus stop', 'Children', 'Crossing (blue)', 'Crossing (red)', 'Cyclists',\n",
        "     'Danger (other)', 'Dangerous left turn', 'Dangerous right turn', 'Give way', 'Go ahead', 'Go ahead or left',\n",
        "     'Go ahead or right', 'Go around either way', 'Go around left', 'Go around right', 'Intersection', 'Limit 100',\n",
        "     'Limit 120', 'Limit 20', 'Limit 30', 'Limit 50', 'Limit 60', 'Limit 70', 'Limit 80', 'Limit 80 over',\n",
        "     'Limit over', 'Main road', 'Main road over', 'Multiple dangerous turns', 'Narrow road (left)',\n",
        "     'Narrow road (right)', 'No entry', 'No entry (both directions)', 'No entry (truck)', 'No stopping', 'No takeover',\n",
        "     'No takeover (truck)', 'No takeover (truck) end', 'No takeover end', 'No waiting', 'One way road',\n",
        "     'Parking','Road works', 'Roundabout', 'Slippery road', 'Stop','Traffic light', 'Train crossing',\n",
        "     'Train crossing (no barrier)','Wild animals','X - Priority','X - Turn Left','X - Turn Right'],\n",
        "    ['SUV','truck','plane'],\n",
        "    ['happy','sad','angry','evil']]\n",
        "\n",
        "myFolderList = sorted([f.path for f in os.scandir(path) if f.is_dir()])\n",
        "print(myFolderList)\n",
        "if str(path + '__pycache__') in myFolderList:\n",
        "  myFolderList.remove(str(path + '__pycache__'))\n",
        "\n",
        "layer_names = net.getLayerNames()\n",
        "output_layers = [layer_names[i[0] - 1] for i in net.getUnconnectedOutLayers()]\n",
        "colors = np.random.uniform(0, 255, size=(len(classes), 3))\n",
        "\n",
        "# Loop through all the images\n",
        "full_dict = {}\n",
        "for f in myFolderList:\n",
        "  images_path = sorted(os.listdir( f + \"/rgb/\"))\n",
        "  depth_path = sorted(os.listdir( f + \"/depth/\"))\n",
        "\n",
        "  for img_path in range(len(images_path)):# images_path:\n",
        "      current_img_dict = {}\n",
        "\n",
        "      # Load image\n",
        "      img = cv2.imread( f + \"/rgb/\" + images_path[img_path])\n",
        "      depth_img = cv2.imread( f + \"/depth/\" + depth_path[img_path], -1)\n",
        "      height, width, channels = img.shape\n",
        "\n",
        "      # Detecting objects\n",
        "      blob = cv2.dnn.blobFromImage(img, 0.00392, (640, 480), (0, 0, 0), True, crop=False)\n",
        "\n",
        "      net.setInput(blob)\n",
        "      outs = net.forward(output_layers)\n",
        "\n",
        "      # Showing informations on the screen\n",
        "      class_ids = []\n",
        "      confidences = []\n",
        "      boxes = []\n",
        "      list_3d = []\n",
        "      centers = []\n",
        "      for out in outs:\n",
        "          for detection in out:\n",
        "              scores = detection[5:]\n",
        "              class_id = np.argmax(scores)\n",
        "              confidence = scores[class_id]\n",
        "              if confidence > 0.3:\n",
        "                  # Object detected\n",
        "                  center_x = int(detection[0] * width)\n",
        "                  center_y = int(detection[1] * height)\n",
        "                  centers.append([center_x, center_y])\n",
        "\n",
        "                  w = int(detection[2] * width)\n",
        "                  h = int(detection[3] * height)\n",
        "\n",
        "                  # Rectangle coordinates\n",
        "                  x = int(center_x - w / 2)\n",
        "                  y = int(center_y - h / 2)\n",
        "\n",
        "                  u = center_x\n",
        "                  v = -(center_y - 480)\n",
        "\n",
        "                  sum = 0\n",
        "                  cntr = 0\n",
        "                  for y1 in range(7):\n",
        "                    for x1 in range(7):\n",
        "                      try:\n",
        "                        z = float(depth_img[center_y-3+y1, center_x-3+x1])\n",
        "                        sum = sum + z\n",
        "                        cntr = cntr + 1\n",
        "                      except:\n",
        "                        pass\n",
        "                  z_avg = sum/cntr\n",
        "                  z = float(z_avg / 1000)\n",
        "\n",
        "                  x1, y1 = fsolve(equations, (1, 1))\n",
        "                  list_3d.append([x1, y1, z])\n",
        "                  boxes.append([x, y, w, h])\n",
        "                  confidences.append(float(confidence))\n",
        "                  class_ids.append(class_id)\n",
        "\n",
        "      indexes = cv2.dnn.NMSBoxes(boxes, confidences, 0.2, 0.2)\n",
        "\n",
        "      objects = [] #list for all predictions predictions\n",
        "      current = [] # buffer for one object\n",
        "      poses = []\n",
        "\n",
        "      font = cv2.FONT_HERSHEY_PLAIN\n",
        "\n",
        "      #no object detected\n",
        "      if len(boxes) == 0:\n",
        "        for i in range(9):\n",
        "          objects.append(0)\n",
        "\n",
        "      for i in range(len(boxes)):\n",
        "          if i in indexes:\n",
        "              x, y, w, h= boxes[i]\n",
        "              x1, y1, z = list_3d[i]\n",
        "              label = str(classes[class_ids[i]])\n",
        "              color = colors[class_ids[i]]\n",
        "              \n",
        "              if x < 0 :\n",
        "                x =0\n",
        "              if y < 0:\n",
        "                y = 0\n",
        "\n",
        "              if label == 'angry' or label == 'evil' or label =='happy' or label =='sad':\n",
        "                classID = 2\n",
        "                subclassID = subclassNames[2].index(label)\n",
        "\n",
        "              elif label == \"SUV\" or label ==\"plane\" or label ==\"truck\": \n",
        "                classID = 1\n",
        "                subclassID = subclassNames[1].index(label)\n",
        "\n",
        "              elif label == \"traffic sign\":\n",
        "                classID = 0                \n",
        "                label = predict_traffic(model, x,y,w,h, img, classes)\n",
        "                subclassID = subclassNames[0].index(label)\n",
        "\n",
        "              #cv2.rectangle(img, (x, y), (x + w, y + h), color, 2)\n",
        "              #cv2.putText(img, label, (x, y + 30), font, 1, color, 1)\n",
        "\n",
        "              #create dictionary for results\n",
        "              current.append(centers[i][0])\n",
        "              current.append(centers[i][1])\n",
        "              current.append(w)\n",
        "              current.append(h)\n",
        "              current.append(classID)\n",
        "              current.append(subclassID)\n",
        "              current.append(float(x1))\n",
        "              current.append(float(y1))\n",
        "              current.append(float(z))\n",
        "\n",
        "              objects.append(current)\n",
        "              current = []\n",
        "\n",
        "      poses = [0.,  0.,  0., 0.,  0.,  0.,  0., 0.,  0.,  0.,  0., 0.]\n",
        "      current_img_dict['poses'] = poses\n",
        "      current_img_dict['objects'] = objects\n",
        "      full_dict[f[9:] + \"/rgb/\" + images_path[img_path]] = current_img_dict\n",
        "\n",
        "#print(full_dict)\n",
        "filename = \"predictions.pickle\"\n",
        "import pickle\n",
        "pickle.dump( full_dict, open( filename, \"wb\" ) )\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HAus2DeVHgWA",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import pickle\n",
        "from HW.evaluate import evaluate\n",
        "\n",
        "file = open('/content/predictions.pickle','rb')\n",
        "predictions = pickle.load(file)\n",
        "evaluate(predictions)"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}