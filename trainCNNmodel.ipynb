{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "CVS_HW.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ul0rvDQ2ZtmZ",
        "colab_type": "text"
      },
      "source": [
        "# **Computer Vision Systems Homework**\n",
        "\n",
        "Hi all!\n",
        "\n",
        "In the spirit of #StayAtHome I tried to assemble a homework project without leaving the house. So, if this looks a little DIY, that's why. So without further ado:\n",
        "\n",
        "## **Welcome to Cactusville!**\n",
        "\n",
        "Cactusville is a small town populated by - you guessed correcty - cacti. Since it is a rapidly developing village, they are considering to use self-driving vehicles in their hometown. Your job as a computer vision maestro is to develop the required detection methods.\n",
        "\n",
        "## The setting\n",
        "\n",
        "Cactusville is quite unique in the sense that the entire surface of the town is covered in blue tablecloth. The exact colour and pattern of the cloth may vary slightly.\n",
        "\n",
        "By-and large there are 3 different objects of interest:\n",
        "\n",
        "* **Cacti:** These are the inhabitants of the village, so self-driving cars must be able to detect them to avoid hitting a cactus. Cacti have four basic sub-types: ***Happy***, ***Sad***, ***Angry*** and ***Evil***\n",
        "* **Vehicles:** These are other vechiles you should also avoid colliding with. There are 3 vehicles in Cactusville: An ***SUV***, a ***truck***, and an ***airplane***.\n",
        "* **Traffic Signs:** There are several signs placed all around the town, often multiple ones on a single stand. There are 55 different traffic sign classes, which are not listed here for the sake of brevity.\n",
        "\n",
        "## Tasks\n",
        "\n",
        "The people of Cactusville provided 4 videos for you to develop your algorithms with. Each video consists of several RGB and corresponding depth frames, which are found in the '*rgb*' and '*depth*' subfolders of the video. They are ordered numerically. The depth image is a single-channel, 16-bit image, where the pixel value is the distance of that pixel from the camera in **mm**.\n",
        "\n",
        "The videos also contain a **calibration.yaml** file, which contains the intrinsic parameters of the camera. These are the same for all videos used, so feel free to hardcode the important values into your program.\n",
        "\n",
        "Your team has to complete the following tasks:\n",
        "\n",
        "1.   **Traditional Vision:** Create an algorithm to accurately detect and classify the 3 objects of interest (Cactus, Vehicle, Traffic Sign). You don't have to determine the subclass at this point.\n",
        "2.   **Deep Learning:** Use a deep learning algorithm to classify traffic signs. The package provided includes a training and validation database of 32x32 RGB images.\n",
        "3.   **3D Vision:** Determine the 3D positions of the object of interest relative to the camera. Use the center of an object's bounding box to determine the position on the image.\n",
        "\n",
        "## Hardcore Tasks\n",
        "\n",
        "There are also 3 hardcore tasks for those who like challenges. These aren't particularly difficult, but they take more work and require you to go a little bit beyond the scope of the practicals.\n",
        "\n",
        "1.   **Traditional Vision:** Determine the subclasses of Cacti and Vehicles\n",
        "2.   **Deep Learning:** Of the 55 possible traffic signs, 3 are missing from the training and test datasets. ('*X - Priority*', '*X - Turn left*', '*X - Turn right*') As a result, the neural net trained in task 2 will not be able to classify them properly. Extend your neural network to classify these as well.\n",
        "3.   **3D Vision:** Determine the absolute pose (4x4 transformation matrix) of the camera as it moves throughout the video. You can safely assume that the pose in the first frame of every video is the identity matrix.\n",
        "\n",
        "## Evaluation and Score\n",
        "\n",
        "The basic package also contains annotations (correct answers) in the file **annotations.pickle** and a small python script **evaluate.py** you can use to measure the performance of your algorithm. \n",
        "\n",
        "Your homework score will be computed using the same script, albeit on 2 secret videos that you were not provided. The reason for this is to make sure that your algorithm works in new situations as well. The secret videos use the same 2 tablecloths and 3 vehicles, but the traffic signs and the cacti may be different. Not to mention the illumination.\n",
        "\n",
        "The tasks will be evaluated using the following metrics:\n",
        "\n",
        "* Task 1 - **Average Precision** (AP): This metric is simply the average of **Recall** (nCorrect / nObject) and **Precision** (nCorrect / nPrediction).\n",
        "* Tasks 1 HC, 2 and 2 HC - **Classification accuracy**\n",
        "* Tasks 3 and 3 HC - **RBF error**: This is simple the squared error between the prediction and the correct answer transformed by an RBF (Radial Basis Function) kernel. This means that a perfect answer has a score of 1, a bad answer will result in a score close to 0.\n",
        "\n",
        "### **Answer format**\n",
        "\n",
        "The evaluation function takes a single argument: A dictionary that containes your predictions. On the top level this dictionary should look like this:\n",
        "\n",
        "```python\n",
        "myAnswers = {\n",
        "    'video1/rgb/1.jpg' : <<Predictions for the image>>,\n",
        "    'video1/rgb/2.jpg' : <<Predictions for the image>>,\n",
        "    ...\n",
        "    'video4/rgb/10.jpg' : <<Predictions for the image>>,\n",
        "}\n",
        "```\n",
        "It is important that the dictionary key contains the video path, since two videos might have image files with the same name. Also, include all images from all videos in the file (even if you have no predictions), since the evaluation function will look for them! The order of the images does not matter.\n",
        "\n",
        "A prediction for a single image should also be a dictionary with the following format:\n",
        "```python\n",
        "myPred = {\n",
        "    'poses' : [t_11, t12_, t_13, t_14, ..., t_33, t_34],\n",
        "    'objects' : [obj_1, obj_2, ... obj_n]\n",
        "}\n",
        "```\n",
        "The key `poses` contains the first three rows of the transformation matrix (the fourth row is always `[0 0 0 1]`). The key `objects` is a list, each element containing a single object prediction. The order of predictions does not matter. A single object prediction is also a list, containing the following elements:\n",
        "```python\n",
        "myObjects = [u, v, w, h, classInd, subClassInd, x, y, z]\n",
        "```\n",
        "\n",
        "* `(u, v)` are the center coordinates of the object's bounding box, while `(w, h)` are the width and height parameters. All four are expected in pixels @640x480 resolution.\n",
        "* `(x, y, z)` are the 3D coordinates of the object relative to the camera. They are expected in **meters**.\n",
        "* `classInd` is the index of the object class in the list `className` (see below). It is between 0 and 2.\n",
        "* `subClassInd` is the index of the subclass in the appropriate list in `subclassNames` (again, see below). It is between [0-54] for traffic signs, [0-2] for vehicles and [0-3] for cacti.\n",
        "\n",
        "## Rules\n",
        "\n",
        "Here are some important rules and guidelines you have to follow:\n",
        "\n",
        "*   This work is to be done in groups of 3 or 4 people. You can do it with less if you feel confident, but not more.\n",
        "*   Forming/finding a group is your job. Once you have one, 1 person from the group shold write me a message on teams with the names and neptun codes of the members.\n",
        "*   If you can't find a group by Sunday, write me and I'll formulate groups with the remaining people.\n",
        "*   The deadline for the submission is Friday midnight on the 14th week. You can make a late submission until the next Sunday midnight.\n",
        "*   You can opt out of the homework. In this case you will beed to take the midterm exam. This will be done via teams video chat (oral exam). If you want to take this option, write me a message by Sunday.\n",
        "*   To pass the homework, you will have to submit a working solution for the 3 basic tasks. The quality of your predictions has to be significantly better than what is achievable by random guessing.\n",
        "\n",
        "### Offered final grade\n",
        "\n",
        "To qualify for the offered final grade (and to skip the exams), you have to complete at least one of the hardcore tasks. What this final grade will be depends on the quality of the predictions. \n",
        "\n",
        "I cannot specify the criteria exactly at this time, since I don't know how easy/hard this homework is yet. I will, however adhere to the following guidelines:\n",
        "\n",
        "*   I'm planning to offer Good (4) and Excellent (5) final grades.\n",
        "*   Those, who completed all 3 hardcore tasks with high quality are gonna get a 5\n",
        "*   Those, who completed at least 2 hardcore tasks with high quality are gonna get **at least** a 4\n",
        "*   'High quality' is undefined to create a situation in which teams compete\n",
        "*   Also, I want to avoid two situations: a., where the criterion is so hard that only a few people manage to get an offered grade; and b., where it is so easy that everyone gets one.\n",
        "*   My goal is that about 40-50% of all students would get an offered grade, 15-20% getting 5, and 25-30% getting 4. These goals are might change if way more people opt out of homework than I expect.\n",
        "\n",
        "### Ethics\n",
        "\n",
        "Copying entire solutions from online sources or each other is plagiarism, and it will be checked using automated tools. There are things that are perfectly okay, such as:\n",
        "*   Copying small snippets (a few lines) from the OpenCV/PyTorch tutorials or stackoverflow, etc.\n",
        "*   Appropriating code from the practicals (you can copy the entire thing), especially the deep learning one.\n",
        "*   Since what is okay and what isn't is a bit subjective, if you are unsure, ask me.\n",
        "\n",
        "## So, how should we do this?\n",
        "\n",
        "So, how can you do this homework, especially if you haven't done things like this before? Here are a few tips:\n",
        "\n",
        "### Environment\n",
        "\n",
        "For development IDE the easiest is to just use Google Colab. To do this you just have to solve the homework inside this notebook. This is the simplest solution, although it has one drawback: the colab notebook has limited debugging capabilities.\n",
        "\n",
        "If you want something more powerful, I recommend the [PyCharm](https://www.jetbrains.com/pycharm/) IDE, which is a free and pretty powerful Python development tool.\n",
        "\n",
        "If you are planning to use PyCharm on Windows, you need to install a Python distribution, since Windows still doesn't come with one (it's 20 effing 20, Microsoft!). I recommend [Anaconda](https://www.anaconda.com/distribution/). Make sure you use Python 3.x and not 2.7.\n",
        "\n",
        "[Here's a tutorial on how to set it up.](https://www.youtube.com/watch?v=e53lRPmWrMI)\n",
        "\n",
        "### Collaboration within the team\n",
        "\n",
        "Since I would strongly discourage teams to collaborate physically in the current situation, I would recommend some methods for remote collaboration.\n",
        "\n",
        "* First of all, use Teams or similar methods to communicate.\n",
        "* Second, use git or a similar version control tool to handle multiple team members working on the same project. \n",
        "* I strongly recommend creating a private repository for your homework on [Github](https://github.com/) (since you can add exactly 3 collaborators - including you that's a 4 person team). There, you can also create issues and other nice-to-have features to track you development. Getting some experience with version control is an absolute must for any engineer anyways.\n",
        "\n",
        "Here's a tutorial for git for those who never used something like this before.\n",
        "\n",
        "To use git from a GUI, I recommend [SmartGit](https://www.syntevo.com/smartgit/) or [Git Extensions](http://gitextensions.github.io/).\n",
        "\n",
        "**ProTip:** If you use a Colab notebook, make sure to clear the output cells (especially figures and images) before you commit. Otherwise you'll litter in your repository.\n",
        "\n",
        "[Here is an introduction to git](https://www.freecodecamp.org/news/learn-the-basics-of-git-in-under-10-minutes-da548267cc91/)\n",
        "\n",
        "### Making a submission\n",
        "\n",
        "You can make a submission at the appropriate page in the edu portal. The results and leaderboard will also be published here. The results are evaluated around 8pm (CET), so it's pointless to make multiple submission per day.\n",
        "\n",
        "**Note**: Your submission should be runnable from Colab or PyCharm (if you used any custom libraries, please note it), and it must include the trained neural network model file from task 2. Also, make sure that only the code required for evaluation is ran (you can use a control variable to skip training code).\n",
        "\n",
        "### Further resources\n",
        "\n",
        "[Python tutorials](https://docs.python.org/3/tutorial/)\n",
        "\n",
        "[OpenCV tutorials](https://opencv-python-tutroals.readthedocs.io/en/latest/py_tutorials/py_tutorials.html)\n",
        "\n",
        "[PyTorch tutorials](https://pytorch.org/tutorials/)\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UgWfyt5SdmUn",
        "colab_type": "text"
      },
      "source": [
        "# Solution\n",
        "## Download dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JpJi3x8AZtEV",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Homework dataset\n",
        "!wget http://deeplearning.iit.bme.hu/CVS/HW.zip\n",
        "!unzip -qq HW.zip\n",
        "!rm HW.zip\n",
        "\n",
        "# Traffic Sign Classification set\n",
        "!wget http://deeplearning.iit.bme.hu/CVS/trafficSignsHW.zip\n",
        "!unzip -qq trafficSignsHW.zip\n",
        "!rm trafficSignsHW.zip"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JnnlL5Zp0wG_",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "%cd '/content/trafficSignsHW/testFULL'\n",
        "!unzip -qq Priority.zip\n",
        "!rm Priority.zip\n",
        "\n",
        "!unzip -qq Cyclists.zip\n",
        "!rm Cyclists.zip\n",
        "\n",
        "!unzip -qq TurnLeft.zip\n",
        "!rm TurnLeft.zip\n",
        "\n",
        "!unzip -qq TurnRight.zip\n",
        "!rm TurnRight.zip\n",
        "\n",
        "%cd '/content/trafficSignsHW/trainFULL'\n",
        "!unzip -qq Priority.zip\n",
        "!rm Priority.zip\n",
        "\n",
        "!unzip -qq Cyclists.zip\n",
        "!rm Cyclists.zip\n",
        "\n",
        "!unzip -qq TurnLeft.zip\n",
        "!rm TurnLeft.zip\n",
        "\n",
        "!unzip -qq TurnRight.zip\n",
        "!rm TurnRight.zip\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OSTAlH8td3eD",
        "colab_type": "text"
      },
      "source": [
        "## Folder example\n",
        "\n",
        "Get all subfolders in a directory\n",
        "\n",
        "```\n",
        "import os\n",
        "myFolderList = [f.path for f in os.scandir(path) if f.is_dir()]\n",
        "```\n",
        "\n",
        "Get all files with extension in a directory\n",
        "\n",
        "```\n",
        "import glob\n",
        "import re\n",
        "\n",
        "def sorted_nicely( l ):\n",
        "    \"\"\" Sort the given iterable in the way that humans expect.\"\"\"\n",
        "    convert = lambda text: int(text) if text.isdigit() else text\n",
        "    alphanum_key = lambda key: [ convert(c) for c in re.split('([0-9]+)', key) ]\n",
        "    return sorted(l, key = alphanum_key)\n",
        "\n",
        "names = sorted_nicely(glob.glob1(path, \"*.extension\"))\n",
        "```\n",
        "\n",
        "### Class names\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eHMtqb8G7Lwk",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "classNames = ['traffic sign', 'vehicle', 'cactus']\n",
        "subclassNames = [\n",
        "    ['Bump', 'Bumpy road', 'Bus stop', 'Children', 'Crossing (blue)', 'Crossing (red)', 'Cyclists',\n",
        "     'Danger (other)', 'Dangerous left turn', 'Dangerous right turn', 'Give way', 'Go ahead', 'Go ahead or left',\n",
        "     'Go ahead or right', 'Go around either way', 'Go around left', 'Go around right', 'Intersection', 'Limit 100',\n",
        "     'Limit 120', 'Limit 20', 'Limit 30', 'Limit 50', 'Limit 60', 'Limit 70', 'Limit 80', 'Limit 80 over',\n",
        "     'Limit over', 'Main road', 'Main road over', 'Multiple dangerous turns', 'Narrow road (left)',\n",
        "     'Narrow road (right)', 'No entry', 'No entry (both directions)', 'No entry (truck)', 'No stopping', 'No takeover',\n",
        "     'No takeover (truck)', 'No takeover (truck) end', 'No takeover end', 'No waiting', 'One way road',\n",
        "     'Parking', 'Road works', 'Roundabout', 'Slippery road', 'Stop', 'Traffic light', 'Train crossing',\n",
        "     'Train crossing (no barrier)', 'Wild animals', 'X - Priority', 'X - Turn left', 'X - Turn right'],\n",
        "    ['SUV','truck','plane'],\n",
        "    ['happy','sad','angry','evil']\n",
        "]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aVZdwqSb7dyZ",
        "colab_type": "text"
      },
      "source": [
        "### Display the first images"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qNjLlbEl7hv_",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "colors = [(0,0,255),(255,0,255),(0,255,0)]\n",
        "\n",
        "def drawBBs(BBs, img):\n",
        "    img = cv2.resize(img, (1280, 960))\n",
        "    for BB in BBs:\n",
        "        u = BB[0]*2\n",
        "        v = BB[1]*2\n",
        "        w = BB[2]*2\n",
        "        h = BB[3]*2\n",
        "        c = BB[4]\n",
        "        sc = BB[5]\n",
        "        x = BB[6]\n",
        "        y = BB[7]\n",
        "        z = BB[8]\n",
        "        s = (u - w // 2, v - h // 2)\n",
        "        e = (u + w // 2, v + h // 2)\n",
        "        cv2.rectangle(img, s, e, colors[c], 1)\n",
        "        tl = (s[0], s[1]+15)\n",
        "        bl = (s[0], e[1]-5)\n",
        "        cv2.putText(img,subclassNames[c][sc],tl,cv2.FONT_HERSHEY_COMPLEX_SMALL,0.75,colors[c])\n",
        "        coords = \"(%.2f, %.2f, %.2f)\" % (x,y,z)\n",
        "        cv2.putText(img,coords,bl,cv2.FONT_HERSHEY_COMPLEX_SMALL,0.65,colors[c])\n",
        "    \n",
        "    return img\n",
        "\n",
        "import pickle\n",
        "import cv2\n",
        "import matplotlib.pyplot as plt\n",
        "#This way it doesn't try to open a window un the GUI - works in python notebook\n",
        "%matplotlib inline\n",
        "\n",
        "# Read images\n",
        "img = cv2.imread(\"HW/g1/rgb/1.jpg\")\n",
        "depth = cv2.imread(\"HW/g1/depth/1.png\", -1)\n",
        "\n",
        "# Read annotations\n",
        "file = open('HW/annotations.pickle','rb')\n",
        "annotations = pickle.load(file)\n",
        "\n",
        "# Visualization\n",
        "depth = depth / 5000.0\n",
        "img = drawBBs(annotations[\"HW/g1/rgb/1.jpg\"][\"objects\"], img)\n",
        "img_rgb = cv2.cvtColor(img,cv2.COLOR_BGR2RGB)\n",
        "\n",
        "# Figure with subplots\n",
        "plt.figure(figsize=(30,30))\n",
        "plt.subplot(1,2,1)\n",
        "plt.imshow(img_rgb)\n",
        "plt.subplot(1,2,2)\n",
        "plt.imshow(depth,cmap='gray')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fOTJfoULdtTy",
        "colab_type": "text"
      },
      "source": [
        "# Your Work"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "28EXqOnb0b-M",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!pip install torch torchvision"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9CEkGAwnxKIt",
        "colab_type": "text"
      },
      "source": [
        "Creating CNN model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zdaEt1nFxP0r",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "\n",
        "# Convolutional module (Conv+ReLU+BatchNorm)\n",
        "class Conv(nn.Module):\n",
        "    \n",
        "    # Constructor gets in and output channels and stride\n",
        "    def __init__(self, in_channels, channels, stride=1):\n",
        "        super(Conv, self).__init__()\n",
        "        \n",
        "        # Create 2D Convolution (3x3)\n",
        "        self.conv = nn.Conv2d(in_channels, channels, kernel_size=3, stride=stride, padding=1, bias=False)\n",
        "        \n",
        "        # Create Batchnorm\n",
        "        self.bn = nn.BatchNorm2d(channels)\n",
        "        \n",
        "    # Overwrite forward\n",
        "    def forward(self,x):\n",
        "        # Call the layers in the proper order\n",
        "        return self.bn(torch.relu(self.conv(x)))\n",
        "\n",
        "class ConvNet(nn.Module):\n",
        "    \n",
        "    # Constructor gets channel number of the image and the first filter\n",
        "    def __init__(self, base_channels=16, in_channels=3, num_classes=55):\n",
        "        super(ConvNet, self).__init__()\n",
        "        \n",
        "        # First two filters\n",
        "        self.c11 = Conv(in_channels, base_channels)\n",
        "        self.c12 = Conv(base_channels, base_channels)\n",
        "        \n",
        "        # Downscale using strided convolution and expand channels\n",
        "        self.d1 = Conv(base_channels, base_channels*2, 2)\n",
        "        \n",
        "        # Repeat this 4 times\n",
        "        self.c21 = Conv(base_channels*2, base_channels*2)\n",
        "        self.c22 = Conv(base_channels*2, base_channels*2)\n",
        "        \n",
        "        self.d2 = Conv(base_channels*2, base_channels*4, 2)\n",
        "        \n",
        "        self.c31 = Conv(base_channels*4, base_channels*4)\n",
        "        self.c32 = Conv(base_channels*4, base_channels*4)\n",
        "        \n",
        "        self.d3 = Conv(base_channels*4, base_channels*8, 2)\n",
        "        \n",
        "        self.c41 = Conv(base_channels*8, base_channels*8)\n",
        "        self.c42 = Conv(base_channels*8, base_channels*8)\n",
        "        \n",
        "        self.d4 = Conv(base_channels*8, base_channels*16, 2)\n",
        "        \n",
        "        self.c51 = Conv(base_channels*16, base_channels*16)\n",
        "        self.c52 = Conv(base_channels*16, base_channels*16)\n",
        "        \n",
        "        # Input image is 32x32 -> after 5 downscaling the activation map is 1x1\n",
        "        self.d5 = Conv(base_channels*16, base_channels*32, 2)\n",
        "        \n",
        "        # Classifier is a normal 1x1 convolution that produces num_classes class scores\n",
        "        # This layer does not have BatchNorm of ReLU\n",
        "        self.classifier = nn.Conv2d(base_channels*32,num_classes,kernel_size=1)\n",
        "        \n",
        "    def forward(self,x):\n",
        "        # Class all the layers\n",
        "        x = self.d1(self.c12(self.c11(x)))\n",
        "        x = self.d2(self.c22(self.c21(x)))\n",
        "        x = self.d3(self.c32(self.c31(x)))\n",
        "        x = self.d4(self.c42(self.c41(x)))\n",
        "        x = self.d5(self.c52(self.c51(x)))\n",
        "        \n",
        "        # Squeeze removes dimensions that have only 1 element\n",
        "        # Output of the conv layer is (batch_size x num_classes x 1 x 1)\n",
        "        # After squeeze is becomes (batch_size x num_classes)\n",
        "        return torch.squeeze(self.classifier(x))\n",
        "print(\"finished\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eqIad357xmnS",
        "colab_type": "text"
      },
      "source": [
        "cuda check"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5KdhTH2IxooC",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "haveCuda = torch.cuda.is_available()\n",
        "print(haveCuda)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dba1bJu1s5kn",
        "colab_type": "text"
      },
      "source": [
        "### Data Augmentation"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PX1VVZlwxxcz",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from torchvision import transforms\n",
        "\n",
        "# Necessary transformations: conversion to PyTorch Tensor and normalization\n",
        "# Normalization is performed with channels-wise means and variances computed on ImageNet\n",
        "transform_val = transforms.Compose([\n",
        "    transforms.ToTensor()\n",
        "    #transforms.ColorJitter(brightness=0.6,contrast=0.6,saturation=0.6,hue=0.4),\n",
        "\n",
        "    #transforms.Normalize((0.49139968, 0.48215827, 0.44653124),\n",
        "     #                   (0.24703233, 0.24348505, 0.26158768))\n",
        "])\n",
        "\n",
        "# Train database transform includes data augmentation\n",
        "transform = transforms.Compose([\n",
        "    # Random 32x32 crops (with 4-wide zero padding - this is needed because the \n",
        "    # input is 32x32 so we can't crop a 32x32 region out of it without padding)\n",
        "    transforms.RandomCrop(32,padding=6),\n",
        "    transforms.RandomRotation(10),\n",
        "    # Flips horizontally with p=0.5\n",
        "    \n",
        "    #transforms.RandomHorizontalFlip(),\n",
        "    \n",
        "    # Random perturbance of brightness, contrast and color\n",
        "    transforms.ColorJitter(brightness=0.4,contrast=0.4,saturation=0.4,hue=0.4),\n",
        "    transforms.ToTensor()\n",
        "    #transforms.Normalize((0.49139968, 0.48215827, 0.44653124),\n",
        "    #                     (0.24703233, 0.24348505, 0.26158768))\n",
        "])\n",
        "print(\"finished\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "o0ntpTzo1nXA",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import os\n",
        "import shutil\n",
        "\n",
        "def get_folders(path):\n",
        "  myFolderList = os.listdir(path)\n",
        "  myFolderList.sort()\n",
        "  return myFolderList\n",
        "\n",
        "def create_dataset_labels(path):\n",
        "  folders = get_folders(path)\n",
        "  labels = []\n",
        "  all_img = 0\n",
        "\n",
        "  for i in (folders):\n",
        "    #print(i)\n",
        "    label_count = len(os.listdir(path + '/' + i))\n",
        "    for j in range(label_count):\n",
        "      labels.append(i)\n",
        "      all_img += 1\n",
        "  \n",
        "  return labels\n",
        "\n",
        "train_labels = create_dataset_labels('./trafficSignsHW/trainFULL')\n",
        "test_labels = create_dataset_labels('./trafficSignsHW/testFULL')\n",
        "print(len(train_labels), len(test_labels))\n",
        "#print(create_dataset_labels('./trafficSignsHW/testFULL'))\n",
        "\n",
        "def rename_images(path):\n",
        "    folders = get_folders(path)\n",
        "    for i in (folders):\n",
        "      file_names = get_folders(path + '/' + i)\n",
        "      for j in file_names:\n",
        "        os.rename(path + '/' + i + '/' + j, path + '/' + i + '/'+ i +'_'+ j)\n",
        "\n",
        "shutil.copytree('./trafficSignsHW', './renamed')\n",
        "rename_images('./renamed/trainFULL')\n",
        "rename_images('./renamed/testFULL')\n",
        "#create copy of dataset\n",
        "\n",
        "#function to copy all images to dst\n",
        "def copy_images(mode, source, dst):\n",
        "  folders = get_folders(source)\n",
        "  copy_destination = dst + '/' + mode\n",
        "  if not os.path.exists(copy_destination):\n",
        "    os.makedirs(copy_destination)\n",
        "  for i in (folders):\n",
        "\n",
        "    file_names = get_folders(source + '/' + i)\n",
        "    for j in file_names:\n",
        "      file_to_copy = source + '/' + i + '/' + j\n",
        "      shutil.copy(file_to_copy, copy_destination + '/')\n",
        "\n",
        "copy_images('Train', './renamed/trainFULL', '.')\n",
        "copy_images('Test', './renamed/testFULL', '.')\n",
        "\n",
        "\n",
        "def delete_folder(path):\n",
        "  try:\n",
        "    shutil.rmtree(path)\n",
        "  except OSError as e:\n",
        "    print (\"Error: %s - %s.\" % (e.filename, e.strerror))\n",
        "#delete_folder('./Train')\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GSNBbf6puOTU",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "train_img_names = os.listdir('./Train')\n",
        "test_img_names = os.listdir('./Test')\n",
        "print(len(train_img_names), len(test_img_names))\n",
        "train_img_names.sort()\n",
        "test_img_names.sort()\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QfCdFq7RvPZE",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!pip install pip opencv-python\n",
        "!pip install -U scikit-learn"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eUTT6I6wveb0",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import cv2\n",
        "import numpy as np\n",
        "from sklearn import preprocessing\n",
        "\n",
        "train_imgs = []\n",
        "test_imgs = []\n",
        "\n",
        "#transpose needed to have the expected dimensions \n",
        "for i in (train_img_names):\n",
        "  img = cv2.imread('./Train/' + i)\n",
        "  img = img.transpose(2,0,1)\n",
        "  train_imgs.append(img)\n",
        "\n",
        "for i in (test_img_names):\n",
        "  img = cv2.imread('./Test/' + i)\n",
        "  img = img.transpose(2,0,1)\n",
        "  test_imgs.append(img)\n",
        "print(len(os.listdir('./renamed/trainFULL')))\n",
        "\n",
        "le_train = preprocessing.LabelEncoder()\n",
        "target_train = le_train.fit_transform(train_labels)\n",
        "target_train = np.array(target_train)\n",
        "train_imgs = np.array(train_imgs)\n",
        "train_data_tensor = torch.from_numpy(train_imgs)\n",
        "train_labels_tensor = torch.from_numpy(target_train)\n",
        "train_labels_tensor = train_labels_tensor.long()\n",
        "train_data_tensor = train_data_tensor.float()\n",
        "trainSet = torch.utils.data.TensorDataset(train_data_tensor, train_labels_tensor)\n",
        "print(train_labels_tensor.shape)\n",
        "print(train_data_tensor.shape)\n",
        "#Dimensions should be [size, number of channels, height, width]\n",
        "\n",
        "le_test = preprocessing.LabelEncoder()\n",
        "target_test = le_test.fit_transform(test_labels)\n",
        "target_test = np.array(target_test)\n",
        "test_imgs = np.array(test_imgs)\n",
        "test_data_tensor = torch.from_numpy(test_imgs)\n",
        "test_labels_tensor = torch.from_numpy(target_test)\n",
        "test_data_tensor = test_data_tensor.float()\n",
        "test_labels_tensor = test_labels_tensor.long()\n",
        "\n",
        "testSet = torch.utils.data.TensorDataset(test_data_tensor, test_labels_tensor)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vfHSgHAcBzAT",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import os\n",
        "\n",
        "print(sorted(os.listdir('/content/trafficSignsHW/trainFULL/')))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mwZIx-fyFUId",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import torchvision\n",
        "\n",
        "\n",
        "train_data = torchvision.datasets.ImageFolder('/content/trafficSignsHW/trainFULL',transform=transform)\n",
        "test_data = torchvision.datasets.ImageFolder('/content/trafficSignsHW/testFULL',transform=transform_val)\n",
        "\n",
        "trainLoader = torch.utils.data.DataLoader(train_data, batch_size=32, shuffle=True)\n",
        "testLoader = torch.utils.data.DataLoader(test_data, batch_size=32, shuffle=True)\n",
        "\n",
        "def createNet():\n",
        "    net = ConvNet()\n",
        "    if haveCuda:\n",
        "        net = net.cuda()\n",
        "    return net\n",
        "\n",
        "def createLoss():\n",
        "    return nn.CrossEntropyLoss()\n",
        "\n",
        "from torch import optim\n",
        "\n",
        "# Stochastic Gradient Descent (SGD) optimizer with Nesterov momentum and 0.1 learning rate\n",
        "# Weight decay is the relative weight of the L2 regularization term\n",
        "def createOptimizer():\n",
        "    return optim.SGD(net.parameters(), lr=1e-1, momentum=0.9, nesterov=True, weight_decay=1e-4)\n",
        "\n",
        "# Run for 50 epochs - 1 epoch means the networks sees every training image once\n",
        "numEpoch = 50\n",
        "\n",
        "# Cosine annealing learning rate scheduler - in 50 epochs the lr will become 0.01\n",
        "def createScheduler():\n",
        "    return optim.lr_scheduler.CosineAnnealingLR(optimizer,numEpoch,eta_min=1e-2)\n",
        "\n",
        "from IPython.display import HTML, display\n",
        "\n",
        "def progress(value, max=100):\n",
        "    return HTML(\"\"\"\n",
        "        <progress\n",
        "            value='{value}'\n",
        "            max='{max}',\n",
        "            style='width: 100%'\n",
        "        >\n",
        "            {value}\n",
        "        </progress>\n",
        "    \"\"\".format(value=value, max=max))\n",
        "\n",
        "\n",
        "#TRAIN\n",
        "def train(epoch):\n",
        "\n",
        "    # variables for loss\n",
        "    running_loss = 0.0\n",
        "    correct = 0.0\n",
        "    total = 0\n",
        "\n",
        "    # set the network to train (for batchnorm and dropout)\n",
        "    net.train()\n",
        "\n",
        "    # Create progress bar\n",
        "    bar = display(progress(0, len(trainLoader)), display_id=True)\n",
        "  \n",
        "    # data will contain one minibatch of images and correcponding labels\n",
        "    # When the iteration is finished we have seen every training image once\n",
        "    for i, data in enumerate(trainLoader, 0):\n",
        "        # get the inputs\n",
        "        #print(i)\n",
        "        inputs, labels = data\n",
        "\n",
        "        #Convert to cuda\n",
        "        if haveCuda:\n",
        "            inputs, labels = inputs.cuda(), labels.cuda()\n",
        "\n",
        "        # Clear any previous gradients\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        # Forward\n",
        "        outputs = net(inputs)\n",
        "        # Loss\n",
        "        loss = criterion(outputs, labels)\n",
        "        # Backpropagation\n",
        "        loss.backward()\n",
        "        # Gradient method\n",
        "        optimizer.step()\n",
        "\n",
        "        # Do not include these steps in the computational graph\n",
        "        with torch.no_grad():\n",
        "            # Accumulate loss\n",
        "            running_loss += loss.item()\n",
        "            # Get indices of the largest goodness values\n",
        "            _, predicted = torch.max(outputs, 1)\n",
        "            # Count how many of the predictions equal the labels\n",
        "            correct += predicted.eq(labels).sum().item()\n",
        "            # Accumulate number of total images seen\n",
        "            total += labels.size(0)\n",
        "\n",
        "        # Progress bar\n",
        "        bar.update(progress(i+1, len(trainLoader)))\n",
        "\n",
        "    # return loss and accuracy\n",
        "    tr_loss = running_loss / i\n",
        "    tr_corr = correct / total * 100\n",
        "    print(\"Train epoch %d loss: %.3f correct: %.2f\" % (epoch + 1, running_loss / i, tr_corr))\n",
        "    return tr_loss,tr_corr\n",
        "\n",
        "#TEST\n",
        "def val(epoch):\n",
        "\n",
        "    # variables for loss\n",
        "    running_loss = 0.0\n",
        "    correct = 0.0\n",
        "    total = 0\n",
        "\n",
        "    # set the network to eval  (for batchnorm and dropout)\n",
        "    net.eval()\n",
        "\n",
        "    # Create progress bar\n",
        "    bar = display(progress(0, len(testLoader)), display_id=True)\n",
        "\n",
        "    for i, data in enumerate(testLoader, 0):\n",
        "        # get the inputs\n",
        "        inputs, labels = data\n",
        "\n",
        "        # Convert to cuda\n",
        "        if haveCuda:\n",
        "            inputs, labels = inputs.cuda(), labels.cuda()\n",
        "\n",
        "        # Do not include these steps in the computational graph\n",
        "        with torch.no_grad():\n",
        "            # Forward\n",
        "            outputs = net(inputs)\n",
        "            # Compute loss\n",
        "            loss = criterion(outputs, labels)\n",
        "\n",
        "            # Compute statistics, just like before\n",
        "            running_loss += loss.item()\n",
        "            _, predicted = torch.max(outputs, 1)\n",
        "            correct += predicted.eq(labels).sum().item()\n",
        "            total += labels.size(0)\n",
        "\n",
        "        bar.update(progress(i+1, len(testLoader)))\n",
        "\n",
        "    # return loss and accuracy\n",
        "    val_loss = running_loss / i\n",
        "    val_corr = correct / total * 100\n",
        "    print(\"Test epoch %d loss: %.3f correct: %.2f\" % (epoch + 1, running_loss / i, val_corr))\n",
        "    return val_loss,val_corr\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CQiN8zk7oI7U",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')\n",
        "!ln -s /content/gdrive/My\\ Drive/ /mydrive\n",
        "!ls /mydrive"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uMhRit2cF36c",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Containers for losses and accuracies for every epoch\n",
        "train_accs = []\n",
        "train_losses = []\n",
        "val_accs = []\n",
        "val_losses = []\n",
        "\n",
        "# Best validation accuracy\n",
        "best_acc = 0\n",
        "\n",
        "# Set pseudo-random generator seeds to make multiple runs comparable\n",
        "torch.manual_seed(42)\n",
        "if haveCuda:\n",
        "    torch.cuda.manual_seed(42)\n",
        "    torch.backends.cudnn.deterministic = True\n",
        "    torch.backends.cudnn.benchmark = False\n",
        "\n",
        "# Create net, criterion, optimizer and scheduler\n",
        "# This needs to be done after setting the random seed, \n",
        "# so that the random initialization would be the same\n",
        "net = createNet()\n",
        "criterion = createLoss()\n",
        "optimizer = createOptimizer()\n",
        "scheduler = createScheduler()\n",
        "\n",
        "# For numEpoch epochs\n",
        "for epoch in range(numEpoch):\n",
        "#for epoch in range(10):\n",
        "    \n",
        "    # The with the LR scheduler\n",
        "    scheduler.step()\n",
        "    \n",
        "    # Train\n",
        "    loss,acc = train(epoch)\n",
        "    train_accs.append(acc)\n",
        "    train_losses.append(loss)\n",
        "    \n",
        "    # Validate\n",
        "    loss,acc = val(epoch)\n",
        "    val_accs.append(acc)\n",
        "    val_losses.append(loss)\n",
        "    \n",
        "    # If the current model is better, than the previous best, save it\n",
        "    if acc > best_acc:\n",
        "        print(\"Best Model, Saving\")\n",
        "        best_acc = acc\n",
        "        torch.save(net,\"/content/traffic_sign_model_HC.pth\")\n",
        "        torch.save(net,\"/content/gdrive/My Drive/traffic_sign_model_HC.pth\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pCc3WCBzF6Um",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#PLOT\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline\n",
        "\n",
        "# X coordinate for plotting\n",
        "x = np.arange(numEpoch)\n",
        "\n",
        "plt.figure(figsize=(20,10))\n",
        "\n",
        "# Train is red, validation is blue\n",
        "plt.subplot(1,2,1)\n",
        "plt.plot(x,train_accs,'r')\n",
        "plt.plot(x,val_accs,'b')\n",
        "\n",
        "plt.subplot(1,2,2)\n",
        "plt.plot(x,train_losses,'r')\n",
        "plt.plot(x,val_losses,'b')\n",
        "\n",
        "plt.show()\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rulfcLSxF9yy",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Get a minibatch from the test loader and convert to cuda\n",
        "inputs, labels = next(iter(testLoader))\n",
        "if haveCuda:\n",
        "    inputs, labels = inputs.cuda(), labels.cuda()\n",
        "\n",
        "# forward\n",
        "outputs = net(inputs)\n",
        "\n",
        "# Get predicted class indices\n",
        "_, predicted = torch.max(outputs, 1)\n",
        "\n",
        "# Values used for normalization\n",
        "mean = torch.Tensor((0.49139968, 0.48215827, 0.44653124)).unsqueeze(1).unsqueeze(1)\n",
        "std = torch.Tensor((0.24703233, 0.24348505, 0.26158768)).unsqueeze(1).unsqueeze(1)\n",
        "\n",
        "# Class names\n",
        "\n",
        "classes = ['Bump', 'Bumpy road', 'Bus stop', 'Children', 'Crossing (blue)', 'Crossing (red)', 'Cyclists',\n",
        "     'Danger (other)', 'Dangerous left turn', 'Dangerous right turn', 'Give way', 'Go ahead', 'Go ahead or left',\n",
        "     'Go ahead or right', 'Go around either way', 'Go around left', 'Go around right', 'Intersection', 'Limit 100',\n",
        "     'Limit 120', 'Limit 20', 'Limit 30', 'Limit 50', 'Limit 60', 'Limit 70', 'Limit 80', 'Limit 80 over',\n",
        "     'Limit over', 'Main road', 'Main road over', 'Multiple dangerous turns', 'Narrow road (left)',\n",
        "     'Narrow road (right)', 'No entry', 'No entry (both directions)', 'No entry (truck)', 'No stopping', 'No takeover',\n",
        "     'No takeover (truck)', 'No takeover (truck) end', 'No takeover end', 'No waiting', 'One way road',\n",
        "     'Parking', 'Priority', 'Road works', 'Roundabout', 'Slippery road', 'Stop', 'Traffic light', 'Train crossing',\n",
        "     'Train crossing (no barrier)', 'Turn Left', 'Turn Right','Wild animals']\n",
        "\n",
        "# List of subplots - we'll use 16 images\n",
        "f, axarr = plt.subplots(2, 8,figsize=(20, 5))\n",
        "\n",
        "# For every image-prediction pair\n",
        "for i,(img,pred) in enumerate(zip(inputs,predicted)):\n",
        "    # undo the normalization\n",
        "    img_rescaled = img.cpu() * std + mean\n",
        "    \n",
        "    # Get predicted class name\n",
        "    name = classes[pred.cpu().item()]\n",
        "    \n",
        "    # Permutation needed because in PyTorch the channel dimension comes first,\n",
        "    # but in numpy and opencv it comes last (3x32x32) -> (32x32x3)\n",
        "    axarr[i//8,i%8].imshow(img_rescaled.permute(1,2,0))\n",
        "    \n",
        "    # Set title to class name\n",
        "    axarr[i//8,i%8].set_title(name)\n",
        "    \n",
        "    # Hide grid lines\n",
        "    axarr[i//8,i%8].grid(False)\n",
        "    \n",
        "    # Hide axes ticks\n",
        "    axarr[i//8,i%8].set_xticks([])\n",
        "    axarr[i//8,i%8].set_yticks([])\n",
        "    \n",
        "    # Only do the first 16\n",
        "    if i == 15:\n",
        "        break"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eZmTz3XRdwm1",
        "colab_type": "text"
      },
      "source": [
        "# Evaluation\n",
        "\n",
        "This snippet assumes that the contents of the downloaded zip file are in the HW folder, and that your predictions are in a dictionary called predictions that adheres to the format specified above"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pSRADdkYd0Er",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from HW.evaluate import evaluate\n",
        "\n",
        "#file = open('HW/annotations.pickle','rb')\n",
        "#predictions = pickle.load(file)\n",
        "\n",
        "evaluate(predictions)"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}